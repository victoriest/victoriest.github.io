# 关于自学机器学习的随记
## 缘起
我个人对机器学习领域进行关注是去年初开始的， 当时因为工作需要正在进行图像识别的一些算法研究。 包括一些ocr的识别研究。 当时为了调优一个算法， 需要适应不同扫描文件的质量以及大小，耗费了大量精力。 于是就在想， 有没有更加简便轻松的方法来取代我进行的这种近乎体力活的调优参数的工作。 
当然那时候也是知道机器学习这一概念的， 然而也只是仅仅知道而已， 印象中那是一个数学天才和行业大佬们才玩的起来的一个东西。在像模像样的看了一两天各种google来的文档和paper后， 被n多看不懂的公式和术语击垮了信心， 发现距离实际运用太远， 就作罢了。 但是， 埋下了一个好奇的种子。 
直到去年年中， 手头上的事情算是告一段落， 便又把这些机器学习的资料查出来看看， 又硬着头皮看了几天。 发现里面提到的微积分， 线性代数之类的都忘光了， 然后感叹了一声：“数死早啊！”就去弄了几本高数书慢慢啃去了。 
这一啃就啃到了alphago战胜了李世石（其实也并没有花太多时间去啃数学， 因为懒），随后大街小巷都在谈人工智能， 都在谈机器学习， 深度学习卷积神经网络之类的概念。 就像几年前人人都说要做大数据的势头一样。 但在我印象中，机器学习领域并没有普及到连街边小贩都能明确的知道诸如卷积神经网络，梯度下降之类的概念。 于是我邪恶的内心一边咒骂这帮人在装x， 一边在思考还要不要继续去啃我的高数并加入他们的装x队伍。
关于我说的装x， 后来我看到了一片文章提到了一个名词， 很好的描述了这种情况， 叫精神互撸 ：Mental Mutual Masturbation，说的是一帮人说着一些他们自己也似懂非懂的概念，讨论一些他们自己也似懂非懂的东西，越说越嗨，共同营造智力上升的快感。 
因为好奇感和也想装个x的虚荣心， 怀着要装x就来个实力装x的信念， 让我断断续续学到今天。
然而还是在门外眺望， 苦笑。

## 心得
* 首先, 高数知识并不是太需要, 虽然其中有不少描述算法的数学公式, 但也仅仅是需要你去看懂这些公式所说明的意义. 并不需要你有解一个微分方程的能力. 
* 相对来说比较重要的是线性代数的一些基础知识, 和矩阵的计算方法. 
* 深度学习, 神经网络只是机器学习中近几年发展比较迅速的一个分支.
* 别在还没有建立概念的情况下就学诸如TensorFlow之类的机器学习框架. 
* 入门首选coursera上吴恩达的公开视频(https://www.coursera.org/learn/machine-learning/)

就这多, 我去继续看视频去了